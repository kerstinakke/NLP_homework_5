% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\usepackage{hyperref}
%%% END Article customizations

%%% The "real" document content comes below...

\title{Homework 6}
\author{Kerstin Ã„kke}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section*{Step 1}
The scores from last week are
\begin{verbatim}
Base
  "best_validation_accuracy": 0.3760217983651226,
  "best_validation_precision": 0.30120481927710846,
  "best_validation_recall": 0.17985611510791366,
  "best_validation_f1_measure": 0.2252252252251784,
  "best_validation_loss": 1.4011307784489222
  
Elmo
  "best_validation_accuracy": 0.44232515894641233,
  "best_validation_precision": 0.33613445378151263,
  "best_validation_recall": 0.28776978417266186,
  "best_validation_f1_measure": 0.3100775193797953,
  "best_validation_loss": 1.2954552139554705
  
Bert
  "best_validation_accuracy": 0.44595821980018163,
  "best_validation_precision": 0.4342105263157895,
  "best_validation_recall": 0.4748201438848921,
  "best_validation_f1_measure": 0.45360824742263045,
  "best_validation_loss": 1.231014141014644
\end{verbatim}
I choose BERT to work with, because it had better scores than the other two models. Link to config: \\\href{https://github.com/kerstinakke/NLP_homework_5/blob/master/sst_classifier_bert.jsonnet}{https://github.com/kerstinakke/NLP\_homework\_5/blob/master/sst\_classifier\_bert.jsonnet}

\section*{Step 2}

At the end of the training the training loss was 1.187 and validation loss was 1.305. These scores suggest there is some overfitting, but it's not too serious. 
\\\textbf{Changes:}
\begin{enumerate}  
\item weight\_decay = 0.005; new training loss: 1.192, new validation loss: 1.243
\\The new scores show that overfitting was reduced. Validation loss did not significantly decrease.
\item  weight\_decay = 0.008; new training loss: 1.192, new validation loss: 1.243
\\This result is pretty much the same as after previous change. Other scores were also similar.
\end{enumerate}

I decided to keep the first change. 
\href{https://github.com/kerstinakke/NLP_homework_5/blob/master/sst_classifier_bert\_v2.jsonnet}{https://github.com/kerstinakke/NLP\_homework\_5/blob/master/sst\_classifier\_bert\_v2.jsonnet}

\section*{Step 3}
\href{https://github.com/kerstinakke/NLP_homework_5/blob/master/model/transformer_seq2vec_encoder.py}{https://github.com/kerstinakke/NLP\_homework\_5/blob/master/model/transformer\_seq2vec\_encoder.py}

I ignore mask (set whole mask to ones). I summarize by summing the sequenece into a single vector.

\section*{Step 4}

Validation loss was 1.240 at the end of the training. The score is slightly better compared to previous. Training loss was 1.065, so overfitting increased.  
\href{https://github.com/kerstinakke/NLP_homework_5/blob/master/sst_classifier_trans.jsonnet}{https://github.com/kerstinakke/NLP\_homework\_5/blob/master/sst\_classifier\_trans.jsonnet}
 
\section*{Step 5}

Because the previous model is good, it was hard to guess which parameter would increase the validation score.
a) I chose to reduce the number of attention heads, because changing it should have strong effect on the result. 
b) I expected the validation score to increase.
c) Reducing the number of parameters leads to more general model and reduces overfitting.
d)  
e)
\end{document}
